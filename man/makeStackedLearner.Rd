% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StackedLearner.R
\name{makeStackedLearner}
\alias{makeStackedLearner}
\title{Create a stacked learner object.}
\usage{
makeStackedLearner(base.learners, super.learner = NULL, predict.type = NULL,
  method = "stack.nocv", id = method, use.feat = FALSE,
  resampling = NULL, parset = list())
}
\arguments{
\item{base.learners}{[(list of) \code{\link{Learner}}]\cr
A list of learners created with \code{makeLearner}.}

\item{super.learner}{[\code{\link{Learner} | character(1)}]\cr
The super learner that makes the final prediction based on the base learners.
If you pass a string, the super learner will be created via \code{makeLearner}.
Not used for \code{method = 'average'}. Default is \code{NULL}.}

\item{predict.type}{[\code{character(1)}]\cr
Sets the type of the final prediction for \code{method = 'average'}.
For other methods, the predict type should be set within \code{super.learner}.
If the type of the base learner prediction, which is set up within \code{base.learners}, is
\describe{
 \item{\code{"prob"}}{then \code{predict.type = 'prob'} will use the average of all
 bease learner predictions and \code{predict.type = 'response'} will use
 the class with highest probability as final prediction.}
 \item{\code{"response"}}{then, for classification tasks with \code{predict.type = 'prob'},
 the final prediction will be the relative frequency based on the predicted base learner classes
 and classification tasks with \code{predict.type = 'response'} will use majority vote of the base
 learner predictions to determine the final prediction.
 For regression tasks, the final prediction will be the average of the base learner predictions.}
}}

\item{method}{[\code{character(1)}]\cr
\dQuote{average} for averaging the predictions of the base learners,
\dQuote{best.baseLearner} for choosing only the best base learner selected
by cross validation,
\dQuote{stack.nocv} for building a super learner using the predictions of the base learners,
\dQuote{stack.cv} for building a super learner using crossvalidated predictions of the base learners.
\dQuote{hill.climb} for averaging the predictions of the base learners, with the weights learned from
hill climbing algorithm,
\dQuote{compress} for compressing the model to mimic the predictions of a collection of base learners
while speeding up the predictions and reducing the size of the model and
\dQuote{classif.bs.optimal} for averaging the predictions of the base learners,
with the weights chosen Brier Score optimal, see Fuchs, K., J. Gertheiss, and G. Tutz (2015). Nearest
neighbor ensembles for functional data with interpretable feature selection. Chemometrics and Intelligent
Laboratory Systems 146, 186-197.
Default is \dQuote{stack.nocv},}

\item{id}{[\code{character(1)}]\cr
Id string for object. Used to display object.
Default is \code{method}.}

\item{use.feat}{[\code{logical(1)}]\cr
Whether the original features should also be passed to the super learner.
Not used for \code{method \%in\% c('average', 'hill.climb', 'classif.bs.optimal')}. # meas.bs.optimal
Default is \code{FALSE}.}

\item{resampling}{[\code{\link{ResampleDesc}}]\cr
the resampling strategy for \code{method \%in\% c('stack.cv', 'best.baseLearner'}.
Currently only CV is allowed.
The default \code{NULL} uses 5-fold CV,
the resampling strategy for \code{method = 'classif.bs.optimal'}
Currently only LOO and CV are allowed.
The default \code{NULL} uses LOO.}

\item{parset}{[\code{list}]\cr
the parameters for \code{hill.climb} method, including
\describe{
  \item{\code{replace}}{Whether a base learner can be selected more than once.}
  \item{\code{init}}{Number of best models being included before the selection algorithm.}
  \item{\code{bagprob}}{The proportion of models being considered in one round of selection.}
  \item{\code{bagtime}}{The number of rounds of the bagging selection.}
  \item{\code{metric}}{The result evaluation metric function taking two parameters
  \code{pred} and \code{true}, the smaller the score the better.}
}
the parameters for \code{compress} method, including
\describe{
   \item{k}{the size multiplier of the generated data}
   \item{prob}{the probability to exchange values}
   \item{s}{the standard deviation of each numerical feature}
}
the parameter \code{measure} for \code{best.baseLearner} method which gives
the measure to be used for evaluating the base learners. Defaults to the
default measure for the learner.}
}
\description{
A stacked learner uses predictions of several base learners and fits
a super learner using these predictions as features in order to predict the outcome.
The following stacking methods are available:

 \describe{
  \item{\code{average}}{Averaging of base learner predictions without weights.}
  \item{\code{best.baseLearner}}{Selecting the best base learner according to a given
  \link{measures} using cross validation.}
  \item{\code{stack.nocv}}{Fits the super learner, where in-sample predictions of the base learners are used.}
  \item{\code{stack.cv}}{Fits the super learner, where the base learner predictions are computed
  by crossvalidated predictions (the resampling strategy can be set via the \code{resampling} argument).}
  \item{\code{hill.climb}}{Select a subset of base learner predictions by hill climbing algorithm.}
  \item{\code{compress}}{Train a neural network to compress the model from a collection of base learners.}
  \item{\code{classif.bs.optimal}}{Weighted averaging of base learner predictions with weights
  chosen Brier Score optimal.}
 }
}
\examples{
  # Classification
  data(iris)
  tsk = makeClassifTask(data = iris, target = "Species")
  base = c("classif.rpart", "classif.lda", "classif.svm")
  lrns = lapply(base, makeLearner)
  lrns = lapply(lrns, setPredictType, "prob")
  m = makeStackedLearner(base.learners = lrns,
    predict.type = "prob", method = "hill.climb")
  tmp = train(m, tsk)
  res = predict(tmp, tsk)

  # Regression
  data(BostonHousing, package = "mlbench")
  tsk = makeRegrTask(data = BostonHousing, target = "medv")
  base = c("regr.rpart", "regr.svm")
  lrns = lapply(base, makeLearner)
  m = makeStackedLearner(base.learners = lrns,
    predict.type = "response", method = "compress")
  tmp = train(m, tsk)
  res = predict(tmp, tsk)
}
